{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One dimensional search methods\n",
    "\n",
    "## Golden Section Search (0th Order)\n",
    "\n",
    "Assumptions: \n",
    "- Unimodal (unique global minimum $x*$ in a given range $[a_0, b_0]$)\n",
    "- If $a_1 < a_2 < x*$ then $f(x*) < f(a_2) < f(a_1)$\n",
    "- If $x* < a_1 < a_2$ then $f(x*) < f(a_1) < f(a_2)$\n",
    "\n",
    "Algorithm:\n",
    "  Starting from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction factor = 0.382, starting interval = [1, 2]\n",
      "N = log_[base=1-reduction](target_interval / 2) = log_[base=0.618](0.100) = 4\n",
      "\n",
      "Iteration 0: a_1=1.382, b_1=1.618, f(a_1)=2.661, f(b_1)=2.429, old_interval=['1.000', '2.000'], new_interval=['1.382', '2.000'], interval_size=0.618\n",
      "Iteration 1: a_2=1.618, b_2=1.764, f(a_2)=2.429, f(b_2)=2.344, old_interval=['1.382', '2.000'], new_interval=['1.618', '2.000'], interval_size=0.382\n",
      "Iteration 2: a_3=1.764, b_3=1.854, f(a_3)=2.344, f(b_3)=2.320, old_interval=['1.618', '2.000'], new_interval=['1.764', '2.000'], interval_size=0.236\n",
      "Iteration 3: a_4=1.854, b_4=1.910, f(a_4)=2.320, f(b_4)=2.317, old_interval=['1.764', '2.000'], new_interval=['1.854', '2.000'], interval_size=0.146\n",
      "The value that minimises f is located in the interval ['1.854', '2.000']\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=2)\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "        Define f(x) here\n",
    "    \"\"\"\n",
    "    return x**2 + 4 * math.cos(x)\n",
    "\n",
    "def golden_ratio():\n",
    "    # Used to symmetrically reduce term and make sure only one function evaluation is done per iteration\n",
    "    return (3 - math.sqrt(5)) / 2\n",
    "\n",
    "def golden_section_search(f, a_0, b_0, target_interval=0.1, reduction=golden_ratio()):\n",
    "    \"\"\"\n",
    "        f - function to optimise\n",
    "        a_0 - starting a which is < b_0\n",
    "        b_0 - upper bound on starting interval\n",
    "        target_interval - target difference between b_i - a_i\n",
    "    \"\"\"\n",
    "    # calculate iterations required to solve\n",
    "    print(f'Reduction factor = {reduction:.3f}, starting interval = {[a_0, b_0]}')\n",
    "    intervals = math.ceil(math.log(target_interval / 1, (1 - reduction)))\n",
    "    print(f'N = log_[base=1-reduction](target_interval / 2) = log_[base={1- reduction:.3f}]({target_interval/2:.3f}) = {intervals}')\n",
    "    print()\n",
    "    a_i_m1, b_i_m1 = a_0, b_0\n",
    "    update_a = True \n",
    "    for i in range(intervals):\n",
    "        old_interval = [a_i_m1, b_i_m1]\n",
    "        # Calculate intermediate points\n",
    "        a_i = a_i_m1 + reduction * (b_i_m1 - a_i_m1)\n",
    "        b_i = a_i_m1 + (1 - reduction) * (b_i_m1 - a_i_m1)\n",
    "\n",
    "        # Evaluate at intermediate points (assume if using golden ratio that one function evaluation is done)\n",
    "        f_a_i, f_b_i = f(a_i), f(b_i)\n",
    "        if f_a_i < f_b_i:\n",
    "            # update_a = False\n",
    "            # Update interval\n",
    "            b_i_m1 = b_i\n",
    "        else:\n",
    "            # update_a = True\n",
    "            a_i_m1 = a_i\n",
    "\n",
    "        print(f'Iteration {i}: a_{i + 1}={a_i:.3f}, b_{i + 1}={b_i:.3f}, f(a_{i+1})={f_a_i:.3f}, f(b_{i+1})={f_b_i:.3f}, old_interval={[\"%.3f\" % x for x in old_interval]}, new_interval={[\"%.3f\" % x for x in [a_i_m1, b_i_m1]]}, interval_size={b_i_m1 - a_i_m1:.3f}')\n",
    "\n",
    "    print(f'The value that minimises f is located in the interval {[\"%.3f\" % x for x in [a_i_m1, b_i_m1] ]}')\n",
    "\n",
    "\n",
    "golden_section_search(f, a_0=1, b_0=2, target_interval=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Netwon's Method (2nd Order)\n",
    "\n",
    "1. Minimising a general non-linear function is difficult\n",
    "\n",
    "    $min f(x)$\n",
    "\n",
    "2. Basic idea: minimise a quadratic approximation\n",
    "    \n",
    "    $min q(x)$\n",
    "    \n",
    "    where $q(x) = f(x_k) + f'(x_k)(x - x_k) + \\frac{1}{2}f''(x_k)(x - x_k)^2$\n",
    "\n",
    "3. Minimise quadratic approximation (not necessarily will lead to a minimum)\n",
    "    \n",
    "    $0 = q'(x) = f'(x_k) + f''(x_k)(x - x_k)$\n",
    "\n",
    "4. Use approximate minimiser as new starting point\n",
    "\n",
    "    $x_{k+1} = x_k - \\frac{f'(x_k)}{f''(x_k)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x_1 = -10.989\n",
      "Iteration 2: x_2 = 1.820\n",
      "Iteration 3: x_3 = 1.899\n",
      "Iteration 4: x_4 = 1.896\n",
      "Iteration 5: x_5 = 1.895\n"
     ]
    }
   ],
   "source": [
    "# Increasing powers of x\n",
    "import math \n",
    "\n",
    "def f(x):\n",
    "    return x**2 + 4 * math.cos(x)\n",
    "    \n",
    "def f_(x):\n",
    "    return 2*x - 4 * math.sin(x)\n",
    "\n",
    "def f__(x):\n",
    "    return 2 - 4 * math.cos(x)\n",
    "\n",
    "def newtons_method(x_0, iterations = 10000000, change_threshold=1e-5):\n",
    "    x_minus_one = x_0\n",
    "    x_k = x_minus_one + 2\n",
    "    k = 0\n",
    "    change = 2*change_threshold\n",
    "    while change > change_threshold and k < iterations:\n",
    "        k += 1\n",
    "        x_k = x_minus_one - f_(x_minus_one) / f__(x_minus_one)\n",
    "        change = abs(x_minus_one - x_k)\n",
    "        x_minus_one = x_k\n",
    "        print(f'Iteration {k}: x_{k} = {x_k:.3f}')\n",
    "\n",
    "    \n",
    "newtons_method(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Secant Method (Quasi-Newton Method) 2nd Order\n",
    "\n",
    "Second derivative can be hard to find in practice so we can approximate it with\n",
    "\n",
    "$ f''(x_k) = \\frac{f'(x_k) - f'(x_{k - 1})}{x_k - x_{k - 1}}$\n",
    "\n",
    "So the update becomes\n",
    "\n",
    "$x_{k + 1} = x_k - f'(x_k) \\frac{x_k - x_{k - 1} }{f'(x_k) - f'(x_{k - 1})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x_1 = 0.330\n",
      "Iteration 2: x_2 = 0.593\n",
      "Iteration 3: x_3 = 0.745\n",
      "Iteration 4: x_4 = 0.778\n",
      "Iteration 5: x_5 = 0.781\n",
      "Iteration 6: x_6 = 0.781\n",
      "Iteration 7: x_7 = 0.781\n"
     ]
    }
   ],
   "source": [
    "# Increasing powers of x\n",
    "\n",
    "def f(x):\n",
    "    return x**4 - 14 * x**3 + 60 * x **2 - 70 * x\n",
    "    \n",
    "def f_(x):\n",
    "    return 4 * x**3 - 42 * x**2 + 120 * x - 70\n",
    "\n",
    "def f__(x):\n",
    "    return 12 * x**2 - 84 * x + 120\n",
    "\n",
    "def newtons_method(x__1, x_0, iterations=100000, change_threshold=1e-5):\n",
    "    x_minus_one = x_0\n",
    "    x_minus_two = x__1\n",
    "    x_k = x_minus_one + 2\n",
    "    k = 0\n",
    "    change = 2*change_threshold\n",
    "    while change > change_threshold and k < iterations:\n",
    "        k += 1\n",
    "        x_k = x_minus_one - f_(x_minus_one) * (x_minus_one - x_minus_two) / (f_(x_minus_one) - f_(x_minus_two))\n",
    "        change = abs(x_minus_one - x_k)\n",
    "        x_minus_two = x_minus_one\n",
    "        x_minus_one = x_k\n",
    "        print(f'Iteration {k}: x_{k} = {x_k:.3f}')\n",
    "\n",
    "    \n",
    "newtons_method(-0.5, -0.6, iterations=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descent Method (1st Order)\n",
    "\n",
    "1. Given a point $x_k$\n",
    "2. Derive descent direction $d_k \\in R^n$ \n",
    "\n",
    "    i.e. $\\nabla f(x_k)^T d^k < 0$\n",
    "\n",
    "    Steepest Descent: Compute gradient at $x_k$\n",
    "    \n",
    "    $d_k = - \\nabla f(x_k) $\n",
    "\n",
    "3. Decide on a step-size $\\alpha _k$\n",
    "\n",
    "    Exact search: $ \\alpha _k \\in argmin_{\\alpha >= 0} f(x_k + \\alpha d_k)$\n",
    "\n",
    "    Backtracking and other methods can also be used\n",
    "\n",
    "4. Transition to the next point\n",
    "\n",
    "    $x_{k+1} = x_k + \\alpha _k d_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards a General Newton-Raphson Method\n",
    "\n",
    "Issues so far with 1-D:\n",
    "- Only applicable to single dimension\n",
    "- Algorithm may cycle\n",
    "- It may fail to find a descent direction as we are not checking if min or max approx \n",
    "- It may converge to a saddle point or a local maximum\n",
    "\n",
    "## Multivariate Newton-Raphson (2nd order)\n",
    "\n",
    "1. As in 1-D case we construct a **quadratic** approximation around the current iterated $x_k$\n",
    "\n",
    "$f(x) \\approx f(x_k) + \\nabla f(x_k)^T(x - x_k) + \\frac{1}{2}(x - x_k)^T \\nabla ^2 f(x_k)(x - x_k) \\triangleq q(x)$\n",
    "\n",
    "2. Apply FONC to $q(x)$\n",
    "\n",
    "$ 0 = \\nabla q(x) = \\nabla f(x_k) + \\nabla ^2 f(x_k) (x - x_k)$\n",
    "\n",
    "3. Assume that $\\nabla ^2 f(x_k) \\succsim 0$ (i.e. positive definite), then\n",
    "\n",
    "$ x_{k + 1} = x_k - \\nabla ^2 f(x_k)^{-1} \\nabla f(x_k) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.0\n",
      "<function unary_to_nary.<locals>.nary_operator.<locals>.nary_f at 0x000002615C7C0160>\n",
      "Iteration 1: x_1 = [1.587 -0.159 0.254 0.254]\n",
      "Iteration 2: x_2 = [1.058 -0.106 0.169 0.169]\n",
      "Iteration 3: x_3 = [0.705 -0.071 0.113 0.113]\n",
      "[0.705 -0.071 0.113 0.113]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as np\n",
    "import autograd\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "def newton(f, x0, tol=0.0001, maxiter=10000):\n",
    "    g = autograd.jacobian(f)\n",
    "    h = autograd.hessian(f)\n",
    "\n",
    "    print(g)\n",
    "    x = x0\n",
    "    for k in range(1, maxiter + 1):\n",
    "\n",
    "        gx = np.squeeze(g(x))\n",
    "        hx = np.squeeze(h(x))\n",
    "\n",
    "        delta = np.linalg.multi_dot([np.linalg.inv(hx), gx])\n",
    "\n",
    "        x = x - delta\n",
    "        print(f'Iteration {k}: x_{k} = {x}')\n",
    "        if np.linalg.norm(delta) < tol:\n",
    "            break\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    return (x[0] + 10 * x[1]) ** 2 + 5 * (x[2] - x[3]) ** 2 + (x[1] - 2 * x[2]) ** 4 + 10 * (x[0] - x[3]) ** 4\n",
    "\n",
    "x0 = np.array([3., -1., 0., 1.])\n",
    "print(f(x0))\n",
    "print(newton(f, x0, maxiter=3))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f3e99bb4d026affef2231846313c5e38eed33be62d498a2c73e2ca45154640c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
